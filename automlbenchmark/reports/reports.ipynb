{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark results reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequirements\n",
    "This notebook requires a kernel running Python 3.5+.\n",
    "You can skip this section if the kernel is already configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "#!pip install jupyter_contrib_nbextensions\n",
    "#!jupyter contrib nbextension install --user\n",
    "#!jupyter nbextension enable python-markdown/main\n",
    "#!pip install jupyter_nbextensions_configurator\n",
    "#!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and selection of the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import report\n",
    "import report.config as config\n",
    "from report import create_file, display"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#disabling this cell\n",
    "results_dir = \"./reports\"\n",
    "print(\"current working dir: {}\".format(os.getcwd()))\n",
    "try:\n",
    "    os.chdir(results_dir)\n",
    "except:\n",
    "    pass\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%run ./reports_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading results, formatting and adding columns\n",
    "- `result` is the raw result metric computed from predictions at the end the benchmark.\n",
    "    For classification problems, it is usually `auc` for binomial classification and `logloss` for multinomial classification.\n",
    "- `score` ensures a standard comparison between tasks: **higher is always better**.\n",
    "- `norm_score` is a normalization of `score` on a `[0, 1]` scale, with `{{zero_one_refs[0]}}` score as `0` and `{{zero_one_refs[1]}}` score as `1`.\n",
    "- `imp_result` and `imp_score` for imputed results/scores. Given a task and a framework:\n",
    "    - if **all folds results/scores are missing**, then no imputation occurs, and the result is `nan` for each fold.\n",
    "    - if **only some folds results/scores are missing**, then the missing result is imputed by the `{{impute_missing_with}}` result for this fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report import prepare_results\n",
    "\n",
    "# load result files\n",
    "res = prepare_results(results_files,\n",
    "                      renamings=renamed_frameworks,\n",
    "                      exclusions=excluded_frameworks,\n",
    "                      imputation=impute_missing_with,\n",
    "                      normalization=zero_one_refs)\n",
    "res.results.to_csv(create_file(\"tables\", results_group, \"all_results.csv\"), \n",
    "                   index=False, \n",
    "                   float_format=config.ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from report import render_metadata\n",
    "\n",
    "# tasks = (res.results.groupby(['task', 'type'])['id']\n",
    "#                     .unique()\n",
    "#                     .map(lambda id: id[0]))\n",
    "# display(tasks)\n",
    "\n",
    "render_metadata(res.metadata, \n",
    "                filename=create_file(\"datasets\", results_group, \"metadata.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "done = (res.done.reset_index()\n",
    "                .groupby(['task', 'framework'])['fold']\n",
    "                .unique())\n",
    "display(done, pretty=False)\n",
    "# display(tabulate(done, tablefmt='plain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing or crashed/aborted tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# not_done = pd.DataFrame([(task, framework) for task in res.tasks \n",
    "#                                            for framework in res.frameworks \n",
    "#                                            if (task, framework) not in done.index],\n",
    "#                         columns=['task', 'framework'])\n",
    "# missing = res.results.append(not_done)\\\n",
    "#                      .groupby(['task', 'framework'])['fold']\\\n",
    "#                      .unique()\\\n",
    "#                      .map(sorted_ints)\\\n",
    "#                      .map(lambda arr: sorted(list(set(range(0, nfolds)) - set(arr))))\\\n",
    "#                      .where(lambda values: values.map(lambda arr: len(arr) > 0))\\\n",
    "#                      .dropna()\n",
    "\n",
    "missing = (res.missing.reset_index()\n",
    "                      .groupby(['task', 'framework'])['fold']\n",
    "                      .unique())\n",
    "display(missing, pretty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failing tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# failed = res.results.where(np.isnan(all_results.result))\\\n",
    "#                     .groupby(['task', 'framework'])['fold']\\\n",
    "#                     .unique()\\\n",
    "#                     .map(sorted_ints)\n",
    "\n",
    "failed = (res.failed.reset_index()\n",
    "                    .groupby(['task', 'framework'])['fold']\n",
    "                    .unique())\n",
    "display(failed, pretty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report.analysis import list_outliers\n",
    "\n",
    "display(list_outliers('result', \n",
    "                      results=res.results,\n",
    "#                       results=res.results.loc[res.results.framework=='h2oautoml']\n",
    "                      z_threshold=2.5,\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging using arithmetic mean over fold `result` or `score`.\n",
    "In following summaries, if not mentioned otherwise, the means are computed over imputed results/scores.\n",
    "Given a task and a framework:\n",
    "- if **all folds results/scores are missing**, then no imputation occured, and the mean result is `nan`.\n",
    "- if **only some folds results/scores are missing**, then the amount of imputed results that contributed to the mean are displayed between parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from report import render_summary\n",
    "\n",
    "summary_results = res.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of models trained\n",
    "\n",
    "When available, displays the average amount of models trained by the framework for each dataset.\n",
    "\n",
    "This amount should be interpreted differently for each framework.\n",
    "For example, with *RandomForest*, this amount corresponds to the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_summary('models', \n",
    "               results=summary_results, \n",
    "               filename=\"models_summary.csv\", \n",
    "               float_format=\"%.f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_summary('result', \n",
    "               results=summary_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_summary('imp_result', \n",
    "               results=summary_results,\n",
    "               filename=\"result_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_summary('imp_score', \n",
    "               results=summary_results,\n",
    "               filename=\"score_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_summary('norm_score', \n",
    "               results=summary_results,\n",
    "               filename=\"norm_score_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report import render_leaderboard\n",
    "\n",
    "leaderboard_results = res.results.loc[~res.results.framework.isin(['constantpredictor', 'randomforest'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_leaderboard('imp_score', \n",
    "                   results=leaderboard_results,\n",
    "                   aggregate=True, \n",
    "                   show_imputations=True, \n",
    "                   filename=\"tasks_leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folds leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_leaderboard('score', \n",
    "                   results=res.results,\n",
    "                   filename=\"folds_leaderboard.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report import draw_score_heatmap\n",
    "\n",
    "# heatmap_results = res.results.loc[~res.results.framework.isin(['constantpredictor', 'randomforest'])]\n",
    "heatmap_results = res.results.loc[~res.results.framework.isin(['constantpredictor'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('imp_score',\n",
    "                   results=heatmap_results,\n",
    "                   type_filter='binary', \n",
    "                   metadata=res.metadata,\n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                   filename=\"binary_score_heat.png\",\n",
    "                   center=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('imp_score', \n",
    "                   results=heatmap_results,\n",
    "                   type_filter='multiclass', \n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                   filename=\"multiclass_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('norm_score', \n",
    "                   results=heatmap_results,\n",
    "                   type_filter='binary', \n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                   filename=\"binary_norm_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('norm_score',\n",
    "                   results=heatmap_results,\n",
    "                   type_filter='multiclass', \n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                   filename=\"multiclass_norm_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report import draw_score_parallel_coord\n",
    "\n",
    "# parallel_coord_results = res.results.loc[~res.results.framework.isin(['randomforest'])]\n",
    "parallel_coord_results = res.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('imp_score',\n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='binary', \n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                          ylabel=binary_score_label,\n",
    "                          legend_loc='lower left',\n",
    "                          filename=\"binary_score_parallel_ccord.png\"\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('imp_score',\n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='multiclass',\n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                          ylabel=multiclass_score_label,\n",
    "                          yscale=('symlog', dict(linthreshy=0.5)),\n",
    "                          legend_loc='lower left',\n",
    "                          filename=\"multiclass_score_parallel_ccord.png\"\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('norm_score', \n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='binary', \n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                          legend_loc='lower left',\n",
    "                          filename=\"binary_norm_score_parallel_ccord.png\"\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('norm_score', \n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='multiclass',\n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                          filename=\"multiclass_norm_score_parallel_ccord.png\", \n",
    "                          yscale='symlog',\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report import draw_score_stripplot\n",
    "\n",
    "# scatterplot_results = (res.results.loc[~res.results.framework.isin(['randomforest'])]\n",
    "#                                   .sort_values(by=['framework']))  # sorting for colors consistency\n",
    "scatterplot_results = res.results.sort_values(by=['framework'])  # sorting for colors consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('imp_result', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='binary', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "                     title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                     xlabel=binary_score_label,\n",
    "                     filename=\"binary_results_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('imp_result',\n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='multiclass', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "#                      xbound=(0,10),\n",
    "                     xscale=('symlog', dict(linthreshx=0.5)),\n",
    "                     title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                     xlabel=multiclass_score_label, \n",
    "                     filename=\"multiclass_results_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('norm_score', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='binary', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "                     xbound=(-0.2, 2),\n",
    "                     xscale='linear',\n",
    "                     title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                     filename=\"binary_norm_score_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('norm_score', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='multiclass', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "                     xbound=(-0.2, 2.5),\n",
    "                     xscale='linear',\n",
    "                     title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                     filename=\"multiclass_norm_score_stripplot.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "res.results.loc[(res.results.task.str.contains('jungle'))&(res.results.framework=='tunedrandomforest')];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "done.iloc[done.index.get_level_values('framework').isin(['autosklearn', 'h2oautoml', 'tpot'])]\\\n",
    "    .apply(sorted_ints);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failures = res.failed.groupby(['task', 'fold', 'framework'])['info']\\\n",
    "                     .unique()\n",
    "#display(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "h2o_res = (res.results.loc[(res.results.framework=='h2oautoml')&(res.results.fold==0)]\n",
    "                     [['task', 'result', 'acc']])\n",
    "# display(h2o_res, pretty=False, tab_format='grid')\n",
    "display(h2o_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve37",
   "language": "python",
   "name": "ve37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
